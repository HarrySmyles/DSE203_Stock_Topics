{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OusgbPWTnolE",
    "outputId": "b2b51fea-5c8a-4808-ca24-6842e449faca"
   },
   "outputs": [],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up20PF8Y1VZP",
    "outputId": "6eb2f25d-f078-4ff6-d8d6-38e50018406a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "#needs 3.8 or higher\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cCBsVAcOnV9i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import requests\n",
    "import datetime as dt\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import py_stringmatching as sm\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime as dt\n",
    "from twitter_scrape import TwitterSearchScraper as tscraper\n",
    "\n",
    "import tweet_article_lda as tlda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4J Integrator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class integrates with the Neo4J graph.  It must be instantiated with a uri, username, and password for the graph and closed after.  The only method that needs to be used is create_stock and given a stock ticker, start, and end date it will create the graph structure for that stock and tie it to the existing graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RYao5OTdnwkQ"
   },
   "outputs": [],
   "source": [
    "class App:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_stock(self, ticker, start_date, end_date):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._create_stock, ticker, start_date, end_date)\n",
    "\n",
    "\n",
    "    # Method to create stock node, this will automatically call the creation of all the nodes that create the \n",
    "    # graph structure related to that stock     \n",
    "    @staticmethod\n",
    "    def _create_stock(tx, ticker, start_date, end_date):\n",
    "        print(\"Making Nodes for Stock: \", ticker)\n",
    "        start = time.time()\n",
    "        req = requests.get(\"https://cloud.iexapis.com/stable/stock/\" + ticker + \"/company?token=\" + iex_key)\n",
    "        if req.status_code == 200:\n",
    "            response = req.json()\n",
    "            compName = response['companyName']\n",
    "            compIndustry = response['industry']\n",
    "            \n",
    "            # Search to see if that stock node already exists in database\n",
    "            query = (\n",
    "                \"MATCH (s:Stock) \"\n",
    "                \"WHERE s.ticker = $ticker \"\n",
    "                \"RETURN s.ticker\"\n",
    "            )\n",
    "            result = tx.run(query, ticker=ticker)\n",
    "            \n",
    "            # If the above query returned nothing, then create the node\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"CREATE (s1:Stock { ticker: $ticker, compName: $compName, industry: $compIndustry}) \"\n",
    "                    \"RETURN s1\"\n",
    "                )\n",
    "                result = tx.run(query, ticker=ticker, compName=compName, compIndustry = compIndustry)\n",
    "            \n",
    "            # Now create quotes surrouding that stock node\n",
    "            App._create_quotes(tx, ticker, start_date, end_date)\n",
    "        end = time.time()\n",
    "        print(\"Completed Nodes for Stock: {}, Time Taken: {:.2f}min\".format(ticker, (end-start)/60))\n",
    "        \n",
    "        return\n",
    "\n",
    "    # Returns full company name of a stock that exists in the graph database, given the ticker\n",
    "    @staticmethod\n",
    "    def _search_compname(tx, ticker):\n",
    "        query = (\n",
    "                \"MATCH (s:Stock) \"\n",
    "                \"WHERE s.ticker = $ticker \"\n",
    "                \"RETURN s.compName\"\n",
    "            )\n",
    "        cname = tx.run(query, ticker = ticker).value()[0]\n",
    "        comp_suffix = ['incorporated', 'corporation', 'limited', 'company', 'inc', 'nv', 'ltd', 'corp', 'co', 'llc']\n",
    "        cname = cname.replace('.', '').replace(',', '').lower()\n",
    "        for suff in comp_suffix:\n",
    "            cname = cname.replace(suff, '')\n",
    "        return cname\n",
    "\n",
    "    # Runs any passed query and outputs the result in a csv file\n",
    "    @staticmethod\n",
    "    def _run_query(tx, query):\n",
    "        test_res = tx.run(query)\n",
    "        prop_list = []\n",
    "        for row in test_res.value():\n",
    "            prop_list.append(dict(row.items()))\n",
    "        out_df = pd.DataFrame(prop_list)\n",
    "        out_df.to_csv('query_results.csv')\n",
    "        return\n",
    "\n",
    "    # Create quote nodes for all the days within the given date range.  Depending on the volume will tag some as \"event\" days\n",
    "    @staticmethod\n",
    "    def _create_quotes(tx, ticker, start_date, end_date):\n",
    "        date_range = [start_date + dt.timedelta(days=i) for i in range((end_date - start_date).days)]\n",
    "\n",
    "        # Pull historical prices of stock within date range, utilizes multithreading\n",
    "        with concurrent.futures.ThreadPoolExecutor(8) as executor:\n",
    "            responses = list(executor.map(App._pull_quote, [ticker for i in range(len(date_range))], date_range, [iex_key for i in range(len(date_range))]))\n",
    "        responses = list(filter(None, responses))\n",
    "\n",
    "        # Create Dataframe to hold stock data to filter and find the \"event\" days\n",
    "        cols = ['date', 'open', 'close', 'high', 'low', 'volume', 'ticker', 'day']\n",
    "        q_df = pd.DataFrame(responses, columns=cols)\n",
    "        filt_df = pd.DataFrame(columns=cols)\n",
    "        \n",
    "        #Find day mean for the stock and finds which days are 2x the day means\n",
    "        for day in [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]:\n",
    "            mean = q_df['volume'].loc[q_df['day'] == day].mean()\n",
    "            filt_df = filt_df.append(q_df.loc[(q_df['day'] == day) & (q_df['volume'] > (mean * 2))])\n",
    "\n",
    "        filt_df = filt_df.reset_index()\n",
    "        event_days = list(filt_df['date'])\n",
    "\n",
    "        # Get properties for each quote node that was provided in the API response\n",
    "        for i in range(len(q_df)):\n",
    "            event_date = q_df['date'][i]\n",
    "            op = q_df['open'][i]\n",
    "            close = q_df['close'][i]\n",
    "            high = q_df['high'][i]\n",
    "            low = q_df['low'][i]\n",
    "            volume = int(q_df['volume'][i])\n",
    "            day = q_df['day'][i]\n",
    "            event_tag = 'False'\n",
    "            if event_date in event_days:\n",
    "                event_tag = 'True'\n",
    "            \n",
    "            # Find if that quote already exists in graph\n",
    "            query = (\n",
    "              \"MATCH(q:Quote) \"\n",
    "              \"WHERE q.date = $event_date AND q.ticker = $ticker \"\n",
    "              \"RETURN q.date\"\n",
    "            )\n",
    "            result = tx.run(query, event_date=event_date, ticker=ticker)\n",
    "            \n",
    "            #If it does not already exist create it and create a relationship to stock node\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"CREATE (q:Quote { ticker: $ticker, date: $event_date, open: $op, close: $close, high: $high, low: $low, volume: $volume, day: $day, event: $event_tag}) \"\n",
    "                )\n",
    "                result = tx.run(query, ticker=ticker, event_date = event_date, op = op, close = close, high = high, low = low, volume = volume, day=day, event_tag = event_tag)\n",
    "\n",
    "                query = (\n",
    "                    \"MATCH(q:Quote), (s:Stock) WHERE q.ticker = $ticker AND s.ticker = $ticker AND NOT (q)-[:PriceOf]->(s) \"\n",
    "                    \"CREATE (q)-[r:PriceOf]->(s)\"\n",
    "                )\n",
    "                result = tx.run(query, ticker=ticker)\n",
    "                \n",
    "            #Find articles and posts for the quotes that are considered events   \n",
    "            if event_tag == 'True':\n",
    "                #Create Article Nodes related to event\n",
    "                App._create_articles(tx, ticker, event_date)\n",
    "\n",
    "                #Create Post Nodes related to event\n",
    "                App._create_posts(tx, ticker, event_date)\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _pull_quote(ticker, i_date, api_key):\n",
    "        weekdays = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "        req = requests.get(\"https://cloud.iexapis.com/stable/stock/\"\n",
    "                        + ticker + \"/chart/date/\" + i_date.strftime(\"%Y%m%d\") + \"?chartByDay=true&token=\" + iex_key)\n",
    "        if req.status_code == 200 and (len(req.json()) > 0):\n",
    "            content = req.json()[0]\n",
    "            date = content['date']\n",
    "            op = content['open']\n",
    "            close = content['close']\n",
    "            high = content['high']\n",
    "            low = content['low']\n",
    "            volume = content['volume']\n",
    "            day = weekdays[i_date.weekday()]\n",
    "            return [date, op, close, high, low, volume, ticker.upper(), day]\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # Create a user node\n",
    "    @staticmethod\n",
    "    def _create_user(tx, username):\n",
    "        query = (\n",
    "            \"CREATE (p1:User { username: $username}) \"\n",
    "            \"RETURN p1\"\n",
    "        )\n",
    "        result = tx.run(query, username=username)\n",
    "        return\n",
    "\n",
    "    # Check if the user exists, if not call the _create_user method\n",
    "    @staticmethod\n",
    "    def _check_user(tx, username):\n",
    "        query = (\n",
    "            \"MATCH (u:User) WHERE u.username = $username \"\n",
    "            \"RETURN u\"\n",
    "        )\n",
    "        result = tx.run(query, username=username)\n",
    "        if result.single() == None:\n",
    "            App._create_user(tx, username)\n",
    "        return\n",
    "\n",
    "\n",
    "    #Create Post node for tweets that happen on a particular day, filtering them as well.\n",
    "    @staticmethod\n",
    "    def _create_posts(tx, ticker, date):\n",
    "        df = App._search_tweets(tx, ticker, date)\n",
    "        \n",
    "        # Remove duplicate tweets via stringmatching\n",
    "        al_tok = sm.AlphabeticTokenizer()\n",
    "        cos = sm.Cosine()\n",
    "        drop_list = []\n",
    "        for i, text in enumerate(df['content']):\n",
    "            tw_tok = al_tok.tokenize(text.lower())\n",
    "            for j in range(i+1, len(df['content'])):\n",
    "                tw2_tok = al_tok.tokenize(df['content'][j].lower())\n",
    "                if cos.get_raw_score(tw_tok, tw2_tok) > .85:\n",
    "                    drop_list.append(j)\n",
    "        df = df.drop(drop_list).reset_index(drop=True)\n",
    "\n",
    "        # Extract content from tweets and create nodes\n",
    "        for i in range(len(df)):\n",
    "            tweet_content = df['content'][i]\n",
    "            post_date = df['date'][i].strftime(\"%Y-%m-%d\")\n",
    "            tweet_id = int(df['tweet_id'][i])\n",
    "            username = df['username'][i]\n",
    "            App._check_user(tx, username)\n",
    "\n",
    "            # Check if that post already exists in AuraDB\n",
    "            query = (\n",
    "                \"MATCH (p:Post) WHERE p.tweet_id = $tweet_id \"\n",
    "                \"RETURN p\"\n",
    "            )\n",
    "            result = tx.run(query, tweet_id = tweet_id)\n",
    "            \n",
    "            # If it does not exist create a new node and relationship to the Quote day, and a user node to it\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"MATCH (e:Quote {date: $post_date})-[]-(s:Stock {ticker: $ticker}) \"\n",
    "                    \"MATCH (u:User) WHERE u.username = $username \"\n",
    "                    \"CREATE (p1:Post { tweet_id: $tweet_id, content: $tweet_content, date: $post_date})-[r:RefersTo]->(e) \"\n",
    "                    \"CREATE (u)-[:Posted]->(p1)\"\n",
    "                )\n",
    "                result = tx.run(query, tweet_id = tweet_id, tweet_content=tweet_content, post_date=post_date, ticker = ticker, username = username)\n",
    "            # If it does already exist, then check if it already has a relationship to quote of this date and ticker\n",
    "            else:\n",
    "                query = (\n",
    "                    \"MATCH (p:Post)-[r:RefersTo]-(e:Quote)-[]-(s:Stock {ticker: $ticker}) WHERE p.tweet_id = $tweet_id \"\n",
    "                    \"RETURN r\"\n",
    "                )\n",
    "                result = tx.run(query, tweet_id = tweet_id, ticker = ticker)\n",
    "                # If that relationship does not already exist, form a new relationship to the quote of this date and ticker\n",
    "                if result.single() == None:\n",
    "                    query = (\n",
    "                        \"MATCH (p:Post) WHERE p.tweet_id = $tweet_id \"\n",
    "                        \"MATCH (e:Quote {date: $post_date})-[]-(s:Stock {ticker: $ticker}) \"\n",
    "                        \"CREATE (p)-[:RefersTo]->(e)\"\n",
    "                    )\n",
    "                    result = tx.run(query, tweet_id = tweet_id, ticker=ticker, post_date = post_date)\n",
    "        \n",
    "        # After all of the tweets are found for this date and ticker, find the topic of those tweets and create a topic node or tie to existing one\n",
    "        App._create_topic_tweet(tx, df, ticker, date)\n",
    "        return\n",
    "\n",
    "    # Uses QPI to find tweets for a certain stock on a given date\n",
    "    @staticmethod\n",
    "    def _search_tweets(tx, ticker, search_date):\n",
    "        # Prepare parameters for search string\n",
    "        search_date = dt.datetime.strptime(search_date, '%Y-%m-%d')\n",
    "        search_term = App._search_compname(tx, ticker)\n",
    "        sdate = dt.datetime(month = search_date.month, day = search_date.day, year = search_date.year, tzinfo=dt.timezone.utc)\n",
    "        edate = sdate + dt.timedelta(days=1)\n",
    "\n",
    "        since_date = sdate.strftime(\"%Y-%m-%d\")\n",
    "        until_date = edate.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # String to search on\n",
    "        company_input_string = \"{} since:{} until:{}\".format(search_term, since_date, until_date)\n",
    "\n",
    "        #Dataframe to store results\n",
    "        company_tweets_df = pd.DataFrame()\n",
    "        tweets_list = []\n",
    "        tweet_keys = [\"date\", \"tweet_id\", \"content\", \"username\", \"company\", \"company_ticker\", \"lang\"]\n",
    "        \n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i, tweet in enumerate(tscraper(company_input_string, top=True).get_items()):\n",
    "            if i>100:\n",
    "                break\n",
    "            tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, search_term, ticker, tweet.lang])\n",
    "\n",
    "        tweets_df = pd.DataFrame(tweets_list, columns = tweet_keys)\n",
    "        tweets_df = tweets_df.drop_duplicates(subset=['tweet_id'], keep='last')\n",
    "        return tweets_df\n",
    "\n",
    "\n",
    "    # Check if news source node\n",
    "    @staticmethod\n",
    "    def _create_news_source(tx, name):\n",
    "        # Check if the new source already exists\n",
    "        query = (\n",
    "            \"MATCH (s:Source) \"\n",
    "            \"WHERE s.name = $name \"\n",
    "            \"RETURN s.name\"\n",
    "        )\n",
    "        result = tx.run(query, name=name)\n",
    "        \n",
    "        # If it does not exist, create the new source node\n",
    "        if result.single() == None:\n",
    "            query = (\n",
    "                \"CREATE (s:Source { name: $name}) \"\n",
    "                \"RETURN s\"\n",
    "            )\n",
    "            result = tx.run(query, name=name)\n",
    "        return\n",
    "\n",
    "    # Find articles and create does for them, removing duplicate articles or ones that are not related to stock\n",
    "    @staticmethod\n",
    "    def _create_articles(tx, ticker, date):\n",
    "        # Find company name to use for article searching and filtering\n",
    "        art_date = dt.datetime.strptime(date, '%Y-%m-%d')\n",
    "        compName = App._search_compname(tx, ticker)\n",
    "        \n",
    "        \n",
    "        # List of new sources to derive from\n",
    "        provider_list = ['marketwatch', '4-traders', 'benzinga', 'yahoo', 'onenewspage', 'thestreet', 'americanbankingnews',\n",
    "                         'reuters', 'autoevolution', 'investors', 'indiatimes', 'bnnbloomberg', 'businessinsider', 'channelnewsasia',\n",
    "                         'cnbc', 'motorsport', 'barrons', 'fool', 'wsj', 'usatoday', 'washingtonpost', 'motortrend', 'apnews', 'forbes',\n",
    "                         'investorplace', 'investing', 'themarketsdaily', 'seekingalpha', 'thisismoney', 'investchronicle', 'tickerreport',\n",
    "                         'zacks', 'wfmz', 'chron', 'com-unik', 'newsbreak', 'voanews', 'smarteranalyst', 'cyprus-mail', 'thehour']   \n",
    "        \n",
    "        # Search for articles from API\n",
    "        response = App._pull_article(compName, art_date)\n",
    "        \n",
    "        # Check if there were articles that were found\n",
    "        if response != 0:\n",
    "            al_tok = sm.AlphabeticTokenizer()\n",
    "            acols = ['art_id', 'url', 'content', 'date_published']\n",
    "            art_df = pd.DataFrame(columns=acols)\n",
    "            \n",
    "            # Loops through each article found in response on \n",
    "            for web_page in response:\n",
    "                provider = web_page[\"provider\"][\"name\"]\n",
    "                title = web_page[\"title\"]\n",
    "                description = web_page[\"description\"]\n",
    "                \n",
    "                # Tokenize title, description, and company name to be used to determine if this stock is the main idea of the article\n",
    "                title_tok = al_tok.tokenize(title.lower())\n",
    "                descrip_tok = al_tok.tokenize(description.lower())\n",
    "                compName_tok = al_tok.tokenize(compName)\n",
    "\n",
    "                \n",
    "                if (provider in provider_list):\n",
    "                    # Filter for the company name/ticker within the description title to look for stock as main idea for article\n",
    "                    if (len([sim for sim in compName_tok if sim in title_tok])/len(compName_tok) > 0.3) | (len([sim for sim in compName_tok if sim in descrip_tok])/len(compName_tok) > 0.3) | (ticker.lower() in title_tok) | (ticker.lower() in descrip_tok):\n",
    "                        art_id = web_page[\"id\"]\n",
    "                        url = web_page[\"url\"]\n",
    "                        date_published = web_page[\"datePublished\"][:10]\n",
    "                        \n",
    "                        # Call _create_news_source method\n",
    "                        App._create_news_source(tx, provider)\n",
    "\n",
    "                        # Check of that article already exists in AuraDB\n",
    "                        query = (\n",
    "                            \"MATCH (a:Article) WHERE a.id = $art_id \"\n",
    "                            \"RETURN a\"\n",
    "                        )\n",
    "                        result = tx.run(query, art_id = art_id)\n",
    "\n",
    "                        # If it does not exist, create the node and create relationship from article to Quote and source to Article\n",
    "                        if result.single() == None:\n",
    "                            query = (\n",
    "                                \"MATCH (e:Quote)-[:PriceOf]->(:Stock {ticker: $ticker}) WHERE e.date = $date_published \"\n",
    "                                \"MATCH (s:Source) WHERE s.name = $provider \"\n",
    "                                \"CREATE (a:Article { id: $art_id, url: $url, title: $title, description: $description, date_published: $date_published})-[r:References]->(e) \"\n",
    "                                \"CREATE (s)-[:Published]->(a)\"\n",
    "                            )\n",
    "                            result = tx.run(query, art_id = art_id, url = url, title=title, date=date, description = description, date_published = date_published, provider = provider, ticker = ticker)\n",
    "                        # If article does exist see if a relationship to this quote already exists, and if not create that new relationship\n",
    "                        else:\n",
    "                            query = (\n",
    "                                \"MATCH (a:Article)-[r:References]-(e:Quote)-[]-(s:Stock {ticker: $ticker}) WHERE a.id = $art_id \"\n",
    "                                \"RETURN r\"\n",
    "                            )\n",
    "                            result = tx.run(query, art_id = art_id, ticker = ticker)\n",
    "                            if result.single() == None:\n",
    "                                query = (\n",
    "                                    \"MATCH (a:Article) WHERE a.id = $art_id \"\n",
    "                                    \"MATCH (e:Quote)-[:PriceOf]->(s:Stock {ticker: $ticker}) WHERE e.date = $date_published \"\n",
    "                                    \"CREATE (a)-[r:References]->(e)\"\n",
    "                                )\n",
    "                                result = tx.run(query, art_id = art_id, ticker=ticker, date_published = date_published)\n",
    "                        # Create a column of combination of title and description to use for LDA\n",
    "                        comb_text = title + description\n",
    "                        add_df = pd.DataFrame([[art_id, url, comb_text, date_published]], columns=acols)\n",
    "                        art_df = art_df.append(add_df, ignore_index=True)\n",
    "            # If at the end of the filtering, there is still at least one article, perform LDA to find the topic of the articles that day\n",
    "            if len(art_df) > 0:\n",
    "                App._create_topic_article(tx, art_df, ticker, date)\n",
    "        return\n",
    "    \n",
    "    # Method that call article API to pull articles\n",
    "    def _pull_article(compName, date):\n",
    "        URL = \"https://rapidapi.p.rapidapi.com/api/search/NewsSearchAPI\"\n",
    "        HEADERS = {\n",
    "            \"x-rapidapi-host\": \"contextualwebsearch-websearch-v1.p.rapidapi.com\",\n",
    "            \"x-rapidapi-key\": '2fcde9db2cmsh454104b03a9e375p159a4djsnd00c3e13b8c0' #Enter Article API Key here\n",
    "        }\n",
    "\n",
    "        # Set Parameters for search\n",
    "        query = compName# + \" stock\"\n",
    "        page_number = 1\n",
    "        page_size = 40\n",
    "        auto_correct = True\n",
    "        safe_search = False\n",
    "        with_thumbnails = True\n",
    "        from_published_date = date.strftime(\"%m/%d/%Y\")\n",
    "        to_published_date = (date+dt.timedelta(days=1)).strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        querystring = {\"q\": query,\n",
    "                    \"pageNumber\": page_number,\n",
    "                    \"pageSize\": page_size,\n",
    "                    \"autoCorrect\": auto_correct,\n",
    "                    \"safeSearch\": safe_search,\n",
    "                    \"withThumbnails\": with_thumbnails,\n",
    "                    \"fromPublishedDate\": from_published_date,\n",
    "                    \"toPublishedDate\": to_published_date}\n",
    "\n",
    "        # Search for the articles\n",
    "        req = requests.get(URL, headers=HEADERS, params=querystring)\n",
    "        if req.status_code == 200 and (len(req.json()) > 0):\n",
    "            res = req.json()\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        # Drop articles that are duplicates \n",
    "        drop_list = []\n",
    "        q_tok = sm.QgramTokenizer(qval = 2)\n",
    "        cos = sm.Cosine()\n",
    "        for i, article in enumerate(res['value']):\n",
    "            t_tok = q_tok.tokenize(article['title'].lower())\n",
    "            for j in range(i+1, len(res['value'])):\n",
    "                t2_tok = q_tok.tokenize(res['value'][j]['title'].lower())\n",
    "                if cos.get_raw_score(t_tok, t2_tok) > .92:\n",
    "                    drop_list.append(j)\n",
    "\n",
    "        arts = [art for i, art in enumerate(res['value']) if i not in drop_list]\n",
    "\n",
    "        return arts\n",
    "    \n",
    "    # Method to create topic node for tweets\n",
    "    @staticmethod\n",
    "    def _create_topic_tweet(tx, tdf, ticker, post_date):\n",
    "        # Run topic classifier from tweet_article_lda python file\n",
    "        classifier = tlda.Topic_Classification(topic_file=1)\n",
    "        topic_name = classifier.tweet_lda(tdf)\n",
    "        \n",
    "        # After topic is identified, see if it already exists in graph database\n",
    "        query = (\n",
    "            \"MATCH (t:Topic) \"\n",
    "            \"WHERE t.name = $topic_name \"\n",
    "            \"RETURN t.name\"\n",
    "        )\n",
    "        result = tx.run(query, topic_name=topic_name)\n",
    "        \n",
    "        # If it does not exist, create it and connect the quote to the topic\n",
    "        if result.single() == None:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                \"CREATE (t:Topic {name: $topic_name}) \"\n",
    "                \"CREATE (q)-[:TweetAbout]->(t)\"\n",
    "            )\n",
    "            result = tx.run(query, ticker=ticker, post_date=post_date, topic_name=topic_name)\n",
    "        # If it does exist, see if a relationship to that topic already exists, otherwise create the relationship\n",
    "        else:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date})-[a:TweetAbout]-(t:Topic {name: $topic_name}) \"\n",
    "                \"RETURN a\"\n",
    "            )\n",
    "            result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                    \"MATCH (t:Topic {name: $topic_name}) \"\n",
    "                    \"CREATE (q)-[:TweetAbout]->(t)\"\n",
    "                )\n",
    "                result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "        return\n",
    "    \n",
    "    # Method to create topic node for articles\n",
    "    @staticmethod\n",
    "    def _create_topic_article(tx, tdf, ticker, post_date):\n",
    "        # Run topic classifier from tweet_article_lda python file\n",
    "        classifier = tlda.Topic_Classification(topic_file=0)\n",
    "        topic_name = classifier.article_lda(tdf)\n",
    "        \n",
    "        # After topic is identified, see if it already exists in graph database\n",
    "        query = (\n",
    "            \"MATCH (t:Topic) \"\n",
    "            \"WHERE t.name = $topic_name \"\n",
    "            \"RETURN t.name\"\n",
    "        )\n",
    "        result = tx.run(query, topic_name=topic_name)\n",
    "        \n",
    "        # If it does not exist, create it and connect the quote to the topic\n",
    "        if result.single() == None:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                \"CREATE (t:Topic {name: $topic_name}) \"\n",
    "                \"CREATE (q)-[:ArticleAbout]->(t)\"\n",
    "            )\n",
    "            result = tx.run(query, ticker=ticker, post_date=post_date, topic_name=topic_name)\n",
    "        \n",
    "        # If it does exist, see if a relationship to that topic already exists, otherwise create the relationship\n",
    "        else:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date})-[a:ArticleAbout]-(t:Topic {name: $topic_name}) \"\n",
    "                \"RETURN a\"\n",
    "            )\n",
    "            result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                    \"MATCH (t:Topic {name: $topic_name}) \"\n",
    "                    \"CREATE (q)-[:ArticleAbout]->(t)\"\n",
    "                )\n",
    "                result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "        return\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run class to create nodes for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDgewbHbn6Ss",
    "outputId": "d0a2c3dc-4d67-4c70-e00f-42b6fa471769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Nodes for Stock:  F\n",
      "Completed Nodes for Stock: F, Time Taken: 7.03min\n",
      "Making Nodes for Stock:  TSLA\n"
     ]
    }
   ],
   "source": [
    "iex_key = 'sk_f4ff43b754ef4e7a9f7d21ce5569ef7c' #Enter IEX CLoud API Key\n",
    "#Neo4j Aura DB Credentials\n",
    "uri = \"neo4j+s://1e76e017.databases.neo4j.io\" #Neo4J URI\n",
    "user = 'neo4j' #Neo4j DB\n",
    "password = 'Y-mcrOhxhLiaxChRli6zcnkAWS5NH6gWP2jQ-3X80uc' #Neo4j DB Password\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = App(uri, user, password)\n",
    "    app.create_stock('F', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('TSLA', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month =11, day = 30, year = 2021))\n",
    "    app.create_stock('GM', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month =11, day = 30, year = 2021))\n",
    "    app.create_stock('STLA', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('HMC', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('RACE', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('TM', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('TTM', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('NIO', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month =11, day = 30, year = 2021))\n",
    "    app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neo4J Integration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
