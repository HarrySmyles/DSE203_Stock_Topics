{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OusgbPWTnolE",
    "outputId": "b2b51fea-5c8a-4808-ca24-6842e449faca"
   },
   "outputs": [],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up20PF8Y1VZP",
    "outputId": "6eb2f25d-f078-4ff6-d8d6-38e50018406a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cCBsVAcOnV9i"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import requests\n",
    "import datetime as dt\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import py_stringmatching as sm\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime as dt\n",
    "from twitter_scrape import TwitterSearchScraper as tscraper\n",
    "\n",
    "import tweet_article_lda as tlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RYao5OTdnwkQ"
   },
   "outputs": [],
   "source": [
    "class App:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_stock(self, ticker, start_date, end_date):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._create_stock, ticker, start_date, end_date)\n",
    "\n",
    "    def create_quotes(self, ticker, start_date, end_date):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._create_quotes, ticker, start_date, end_date)\n",
    "\n",
    "    def create_user(self, username):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._create_user, username)  \n",
    "                \n",
    "    def create_posts(self, df):  #Finding the stock from text would be a function before calling this one\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._create_posts, df)\n",
    "            \n",
    "    def create_topic(self, name):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._create_topic, name)\n",
    "            \n",
    "    def run_query(self, query):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.write_transaction(self._run_query, query)\n",
    "\n",
    "\n",
    "    #Method to create stocks            \n",
    "    @staticmethod\n",
    "    def _create_stock(tx, ticker, start_date, end_date):\n",
    "        print(\"Making Nodes for Stock: \", ticker)\n",
    "        start = time.time()\n",
    "        req = requests.get(\"https://cloud.iexapis.com/stable/stock/\" + ticker + \"/company?token=\" + iex_key)\n",
    "        if req.status_code == 200:\n",
    "            response = req.json()\n",
    "            compName = response['companyName']\n",
    "            compIndustry = response['industry']\n",
    "\n",
    "            query = (\n",
    "                \"MATCH (s:Stock) \"\n",
    "                \"WHERE s.ticker = $ticker \"\n",
    "                \"RETURN s.ticker\"\n",
    "            )\n",
    "            result = tx.run(query, ticker=ticker)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"CREATE (s1:Stock { ticker: $ticker, compName: $compName, industry: $compIndustry}) \"\n",
    "                    \"RETURN s1\"\n",
    "                )\n",
    "                result = tx.run(query, ticker=ticker, compName=compName, compIndustry = compIndustry)\n",
    "\n",
    "            App._create_quotes(tx, ticker, start_date, end_date)\n",
    "        end = time.time()\n",
    "        print(\"Completed Nodes for Stock: {}, Time Taken: {:.2f}min\".format(ticker, (end-start)/60))\n",
    "        \n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _search_compname(tx, ticker):\n",
    "        query = (\n",
    "                \"MATCH (s:Stock) \"\n",
    "                \"WHERE s.ticker = $ticker \"\n",
    "                \"RETURN s.compName\"\n",
    "            )\n",
    "        cname = tx.run(query, ticker = ticker).value()[0]\n",
    "        comp_suffix = ['incorporated', 'corporation', 'limited', 'company', 'inc', 'nv', 'ltd', 'corp', 'co', 'llc']\n",
    "        cname = cname.replace('.', '').replace(',', '').lower()\n",
    "        for suff in comp_suffix:\n",
    "            cname = cname.replace(suff, '')\n",
    "        return cname\n",
    "\n",
    "    @staticmethod\n",
    "    def _run_query(tx, query):\n",
    "        test_res = tx.run(query)\n",
    "        prop_list = []\n",
    "        for row in test_res.value():\n",
    "            prop_list.append(dict(row.items()))\n",
    "        out_df = pd.DataFrame(prop_list)\n",
    "        out_df.to_csv('query_results.csv')\n",
    "        return\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_quotes(tx, ticker, start_date, end_date):\n",
    "        date_range = [start_date + dt.timedelta(days=i) for i in range((end_date - start_date).days)]\n",
    "        #for res in responses:\n",
    "        with concurrent.futures.ThreadPoolExecutor(8) as executor:\n",
    "            responses = list(executor.map(App._pull_quote, [ticker for i in range(len(date_range))], date_range, [iex_key for i in range(len(date_range))]))\n",
    "        responses = list(filter(None, responses))\n",
    "\n",
    "        cols = ['date', 'open', 'close', 'high', 'low', 'volume', 'ticker', 'day']\n",
    "        q_df = pd.DataFrame(responses, columns=cols)\n",
    "\n",
    "        filt_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "        for day in [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]:\n",
    "            mean = q_df['volume'].loc[q_df['day'] == day].mean()\n",
    "            filt_df = filt_df.append(q_df.loc[(q_df['day'] == day) & (q_df['volume'] > (mean * 2))])\n",
    "\n",
    "        filt_df = filt_df.reset_index()\n",
    "        event_days = list(filt_df['date'])\n",
    "\n",
    "        for i in range(len(q_df)):\n",
    "            event_date = q_df['date'][i]\n",
    "            op = q_df['open'][i]\n",
    "            close = q_df['close'][i]\n",
    "            high = q_df['high'][i]\n",
    "            low = q_df['low'][i]\n",
    "            volume = int(q_df['volume'][i])\n",
    "            day = q_df['day'][i]\n",
    "            event_tag = 'False'\n",
    "            if event_date in event_days:\n",
    "                event_tag = 'True'\n",
    "\n",
    "            query = (\n",
    "              \"MATCH(q:Quote) \"\n",
    "              \"WHERE q.date = $event_date AND q.ticker = $ticker \"\n",
    "              \"RETURN q.date\"\n",
    "            )\n",
    "            result = tx.run(query, event_date=event_date, ticker=ticker)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"CREATE (q:Quote { ticker: $ticker, date: $event_date, open: $op, close: $close, high: $high, low: $low, volume: $volume, day: $day, event: $event_tag}) \"\n",
    "                )\n",
    "                result = tx.run(query, ticker=ticker, event_date = event_date, op = op, close = close, high = high, low = low, volume = volume, day=day, event_tag = event_tag)\n",
    "\n",
    "                query = (\n",
    "                    \"MATCH(q:Quote), (s:Stock) WHERE q.ticker = $ticker AND s.ticker = $ticker AND NOT (q)-[:PriceOf]->(s) \"\n",
    "                    \"CREATE (q)-[r:PriceOf]->(s)\"\n",
    "                )\n",
    "                result = tx.run(query, ticker=ticker)\n",
    "            if event_tag == 'True':\n",
    "                #Create Article Nodes related to event\n",
    "                App._create_articles(tx, ticker, event_date)\n",
    "\n",
    "                #Create Post Nodes related to event\n",
    "                App._create_posts(tx, ticker, event_date)\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _pull_quote(ticker, i_date, api_key):\n",
    "        weekdays = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "        req = requests.get(\"https://cloud.iexapis.com/stable/stock/\"\n",
    "                        + ticker + \"/chart/date/\" + i_date.strftime(\"%Y%m%d\") + \"?chartByDay=true&token=\" + iex_key)\n",
    "        if req.status_code == 200 and (len(req.json()) > 0):\n",
    "            content = req.json()[0]\n",
    "            date = content['date']\n",
    "            op = content['open']\n",
    "            close = content['close']\n",
    "            high = content['high']\n",
    "            low = content['low']\n",
    "            volume = content['volume']\n",
    "            day = weekdays[i_date.weekday()]\n",
    "            return [date, op, close, high, low, volume, ticker.upper(), day]\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_user(tx, username):\n",
    "        query = (\n",
    "            \"CREATE (p1:User { username: $username}) \"\n",
    "            \"RETURN p1\"\n",
    "        )\n",
    "        result = tx.run(query, username=username)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_user(tx, username):\n",
    "        query = (\n",
    "            \"MATCH (u:User) WHERE u.username = $username \"\n",
    "            \"RETURN u\"\n",
    "        )\n",
    "        result = tx.run(query, username=username)\n",
    "        if result.single() == None:\n",
    "            App._create_user(tx, username)\n",
    "        return\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_posts(tx, ticker, date):\n",
    "        df = App._search_tweets(tx, ticker, date)\n",
    "        \n",
    "        # Remove duplicate tweets via stringmatching\n",
    "        al_tok = sm.AlphabeticTokenizer()\n",
    "        cos = sm.Cosine()\n",
    "        drop_list = []\n",
    "        for i, text in enumerate(df['content']):\n",
    "            tw_tok = al_tok.tokenize(text.lower())\n",
    "            for j in range(i+1, len(df['content'])):\n",
    "                tw2_tok = al_tok.tokenize(df['content'][j].lower())\n",
    "                if cos.get_raw_score(tw_tok, tw2_tok) > .85:\n",
    "                    drop_list.append(j)\n",
    "        df = df.drop(drop_list).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            tweet_content = df['content'][i]\n",
    "            post_date = df['date'][i].strftime(\"%Y-%m-%d\")\n",
    "            tweet_id = int(df['tweet_id'][i])\n",
    "            username = df['username'][i]\n",
    "            App._check_user(tx, username)\n",
    "\n",
    "            #Check if that post already exists in AuraDB\n",
    "            query = (\n",
    "                \"MATCH (p:Post) WHERE p.tweet_id = $tweet_id \"\n",
    "                \"RETURN p\"\n",
    "            )\n",
    "            result = tx.run(query, tweet_id = tweet_id)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"MATCH (e:Quote {date: $post_date})-[]-(s:Stock {ticker: $ticker}) \"\n",
    "                    \"MATCH (u:User) WHERE u.username = $username \"\n",
    "                    \"CREATE (p1:Post { tweet_id: $tweet_id, content: $tweet_content, date: $post_date})-[r:RefersTo]->(e) \"\n",
    "                    \"CREATE (u)-[:Posted]->(p1)\"\n",
    "                )\n",
    "                result = tx.run(query, tweet_id = tweet_id, tweet_content=tweet_content, post_date=post_date, ticker = ticker, username = username)\n",
    "            else:\n",
    "                query = (\n",
    "                    \"MATCH (p:Post)-[r:RefersTo]-(e:Quote)-[]-(s:Stock {ticker: $ticker}) WHERE p.tweet_id = $tweet_id \"\n",
    "                    \"RETURN r\"\n",
    "                )\n",
    "                result = tx.run(query, tweet_id = tweet_id, ticker = ticker)\n",
    "                if result.single() == None:\n",
    "                    query = (\n",
    "                        \"MATCH (p:Post) WHERE p.tweet_id = $tweet_id \"\n",
    "                        \"MATCH (e:Quote {date: $post_date})-[]-(s:Stock {ticker: $ticker}) \"\n",
    "                        \"CREATE (p)-[:RefersTo]->(e)\"\n",
    "                    )\n",
    "                    result = tx.run(query, tweet_id = tweet_id, ticker=ticker, post_date = post_date)  \n",
    "        App._create_topic_tweet(tx, df, ticker, date)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def _search_tweets(tx, ticker, search_date):\n",
    "        search_date = dt.datetime.strptime(search_date, '%Y-%m-%d')\n",
    "        search_term = App._search_compname(tx, ticker)\n",
    "        sdate = dt.datetime(month = search_date.month, day = search_date.day, year = search_date.year, tzinfo=dt.timezone.utc)\n",
    "        edate = sdate + dt.timedelta(days=1)\n",
    "\n",
    "        since_date = sdate.strftime(\"%Y-%m-%d\")\n",
    "        until_date = edate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        company_tweets_df = pd.DataFrame()\n",
    "        tweets_list = []\n",
    "        tweet_keys = [\"date\", \"tweet_id\", \"content\", \"username\", \"company\", \"company_ticker\", \"lang\"]\n",
    "\n",
    "        company_input_string = \"{} since:{} until:{}\".format(search_term, since_date, until_date)\n",
    "\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i, tweet in enumerate(tscraper(company_input_string, top=True).get_items()):\n",
    "            if i>100:\n",
    "                break\n",
    "            tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, search_term, ticker, tweet.lang])\n",
    "\n",
    "        tweets_df = pd.DataFrame(tweets_list, columns = tweet_keys)\n",
    "        # Gets rid of duplicate tweets (by tweet_id) & keep only last instance\n",
    "        tweets_df = tweets_df.drop_duplicates(subset=['tweet_id'], keep='last')\n",
    "        return tweets_df\n",
    "\n",
    "\n",
    "#Check if news source exists, and if not create the node\n",
    "    @staticmethod\n",
    "    def _create_news_source(tx, name):\n",
    "        query = (\n",
    "            \"MATCH (s:Source) \"\n",
    "            \"WHERE s.name = $name \"\n",
    "            \"RETURN s.name\"\n",
    "        )\n",
    "        result = tx.run(query, name=name)\n",
    "        if result.single() == None:\n",
    "            query = (\n",
    "                \"CREATE (s:Source { name: $name}) \"\n",
    "                \"RETURN s\"\n",
    "            )\n",
    "            result = tx.run(query, name=name)\n",
    "        return\n",
    "\n",
    "\n",
    "    def _create_articles(tx, ticker, date):\n",
    "        art_date = dt.datetime.strptime(date, '%Y-%m-%d')\n",
    "        compName = App._search_compname(tx, ticker)\n",
    "        al_tok = sm.AlphabeticTokenizer()\n",
    "        \n",
    "        provider_list = ['marketwatch', '4-traders', 'benzinga', 'yahoo', 'onenewspage', 'thestreet', 'americanbankingnews',\n",
    "                         'reuters', 'autoevolution', 'investors', 'indiatimes', 'bnnbloomberg', 'businessinsider', 'channelnewsasia',\n",
    "                         'cnbc', 'motorsport', 'barrons', 'fool', 'wsj', 'usatoday', 'washingtonpost', 'motortrend', 'apnews', 'forbes',\n",
    "                         'investorplace', 'investing', 'themarketsdaily', 'seekingalpha', 'thisismoney', 'investchronicle', 'tickerreport',\n",
    "                         'zacks', 'wfmz', 'chron', 'com-unik', 'newsbreak', 'voanews', 'smarteranalyst', 'cyprus-mail', 'thehour']\n",
    "        \n",
    "        response = App._pull_article(compName, art_date)\n",
    "        if response != 0:\n",
    "            acols = ['art_id', 'url', 'content', 'date_published']\n",
    "            art_df = pd.DataFrame(columns=acols)\n",
    "            for web_page in response:\n",
    "                provider = web_page[\"provider\"][\"name\"]\n",
    "                title = web_page[\"title\"]\n",
    "                description = web_page[\"description\"]\n",
    "\n",
    "                title_tok = al_tok.tokenize(title.lower())\n",
    "                descrip_tok = al_tok.tokenize(description.lower())\n",
    "                compName_tok = al_tok.tokenize(compName)\n",
    "\n",
    "                if (provider in provider_list):\n",
    "                    if (len([sim for sim in compName_tok if sim in title_tok])/len(compName_tok) > 0.3) | (len([sim for sim in compName_tok if sim in descrip_tok])/len(compName_tok) > 0.3) | (ticker.lower() in title_tok) | (ticker.lower() in descrip_tok):\n",
    "                        art_id = web_page[\"id\"]\n",
    "                        url = web_page[\"url\"]\n",
    "                        date_published = web_page[\"datePublished\"][:10]\n",
    "\n",
    "                        App._create_news_source(tx, provider)\n",
    "\n",
    "                        #Check of that article already exists in AuraDB\n",
    "                        query = (\n",
    "                            \"MATCH (a:Article) WHERE a.id = $art_id \"\n",
    "                            \"RETURN a\"\n",
    "                        )\n",
    "                        result = tx.run(query, art_id = art_id)\n",
    "\n",
    "                        if result.single() == None:\n",
    "                            query = (\n",
    "                                \"MATCH (e:Quote)-[:PriceOf]->(:Stock {ticker: $ticker}) WHERE e.date = $date_published \"\n",
    "                                \"MATCH (s:Source) WHERE s.name = $provider \"\n",
    "                                \"CREATE (a:Article { id: $art_id, url: $url, title: $title, description: $description, date_published: $date_published})-[r:References]->(e) \"\n",
    "                                \"CREATE (s)-[:Published]->(a)\"\n",
    "                            )\n",
    "                            result = tx.run(query, art_id = art_id, url = url, title=title, date=date, description = description, date_published = date_published, provider = provider, ticker = ticker)\n",
    "                        else:\n",
    "                            query = (\n",
    "                                \"MATCH (a:Article)-[r:References]-(e:Quote)-[]-(s:Stock {ticker: $ticker}) WHERE a.id = $art_id \"\n",
    "                                \"RETURN r\"\n",
    "                            )\n",
    "                            result = tx.run(query, art_id = art_id, ticker = ticker)\n",
    "                            if result.single() == None:\n",
    "                                query = (\n",
    "                                    \"MATCH (a:Article) WHERE a.id = $art_id \"\n",
    "                                    \"MATCH (e:Quote)-[:PriceOf]->(s:Stock {ticker: $ticker}) WHERE e.date = $date_published \"\n",
    "                                    \"CREATE (a)-[r:References]->(e)\"\n",
    "                                )\n",
    "                                result = tx.run(query, art_id = art_id, ticker=ticker, date_published = date_published)\n",
    "                        comb_text = title + description\n",
    "                        add_df = pd.DataFrame([[art_id, url, comb_text, date_published]], columns=acols)\n",
    "                        art_df = art_df.append(add_df, ignore_index=True)\n",
    "            if len(art_df) > 0:\n",
    "                App._create_topic_article(tx, art_df, ticker, date)\n",
    "        return\n",
    "    \n",
    "    def _pull_article(compName, date):\n",
    "        URL = \"https://rapidapi.p.rapidapi.com/api/search/NewsSearchAPI\"\n",
    "        HEADERS = {\n",
    "            \"x-rapidapi-host\": \"contextualwebsearch-websearch-v1.p.rapidapi.com\",\n",
    "            \"x-rapidapi-key\": '2fcde9db2cmsh454104b03a9e375p159a4djsnd00c3e13b8c0'\n",
    "        }\n",
    "\n",
    "        query = compName# + \" stock\"\n",
    "        page_number = 1\n",
    "        page_size = 40\n",
    "        auto_correct = True\n",
    "        safe_search = False\n",
    "        with_thumbnails = True\n",
    "        from_published_date = date.strftime(\"%m/%d/%Y\")\n",
    "        to_published_date = (date+dt.timedelta(days=1)).strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        querystring = {\"q\": query,\n",
    "                    \"pageNumber\": page_number,\n",
    "                    \"pageSize\": page_size,\n",
    "                    \"autoCorrect\": auto_correct,\n",
    "                    \"safeSearch\": safe_search,\n",
    "                    \"withThumbnails\": with_thumbnails,\n",
    "                    \"fromPublishedDate\": from_published_date,\n",
    "                    \"toPublishedDate\": to_published_date}\n",
    "\n",
    "        req = requests.get(URL, headers=HEADERS, params=querystring)\n",
    "        if req.status_code == 200 and (len(req.json()) > 0):\n",
    "            res = req.json()\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        drop_list = []\n",
    "        q_tok = sm.QgramTokenizer(qval = 2)\n",
    "        cos = sm.Cosine()\n",
    "        for i, article in enumerate(res['value']):\n",
    "            t_tok = q_tok.tokenize(article['title'].lower())\n",
    "            for j in range(i+1, len(res['value'])):\n",
    "                t2_tok = q_tok.tokenize(res['value'][j]['title'].lower())\n",
    "                if cos.get_raw_score(t_tok, t2_tok) > .92:\n",
    "                    drop_list.append(j)\n",
    "\n",
    "        arts = [art for i, art in enumerate(res['value']) if i not in drop_list]\n",
    "\n",
    "        return arts\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_topic_tweet(tx, tdf, ticker, post_date):\n",
    "        classifier = tlda.Topic_Classification(topic_file=1)\n",
    "        topic_name = classifier.tweet_lda(tdf)\n",
    "        query = (\n",
    "            \"MATCH (t:Topic) \"\n",
    "            \"WHERE t.name = $topic_name \"\n",
    "            \"RETURN t.name\"\n",
    "        )\n",
    "        result = tx.run(query, topic_name=topic_name)\n",
    "        if result.single() == None:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                \"CREATE (t:Topic {name: $topic_name}) \"\n",
    "                \"CREATE (q)-[:TweetAbout]->(t)\"\n",
    "            )\n",
    "            result = tx.run(query, ticker=ticker, post_date=post_date, topic_name=topic_name)\n",
    "        else:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date})-[a:TweetAbout]-(t:Topic {name: $topic_name}) \"\n",
    "                \"RETURN a\"\n",
    "            )\n",
    "            result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                    \"MATCH (t:Topic {name: $topic_name}) \"\n",
    "                    \"CREATE (q)-[:TweetAbout]->(t)\"\n",
    "                )\n",
    "                result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_topic_article(tx, tdf, ticker, post_date):\n",
    "        classifier = tlda.Topic_Classification(topic_file=0)\n",
    "        topic_name = classifier.article_lda(tdf)\n",
    "        query = (\n",
    "            \"MATCH (t:Topic) \"\n",
    "            \"WHERE t.name = $topic_name \"\n",
    "            \"RETURN t.name\"\n",
    "        )\n",
    "        result = tx.run(query, topic_name=topic_name)\n",
    "        if result.single() == None:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                \"CREATE (t:Topic {name: $topic_name}) \"\n",
    "                \"CREATE (q)-[:ArticleAbout]->(t)\"\n",
    "            )\n",
    "            result = tx.run(query, ticker=ticker, post_date=post_date, topic_name=topic_name)\n",
    "        else:\n",
    "            query = (\n",
    "                \"MATCH (q:Quote {ticker: $ticker, date: $post_date})-[a:ArticleAbout]-(t:Topic {name: $topic_name}) \"\n",
    "                \"RETURN a\"\n",
    "            )\n",
    "            result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "            if result.single() == None:\n",
    "                query = (\n",
    "                    \"MATCH (q:Quote {ticker: $ticker, date: $post_date}) \"\n",
    "                    \"MATCH (t:Topic {name: $topic_name}) \"\n",
    "                    \"CREATE (q)-[:ArticleAbout]->(t)\"\n",
    "                )\n",
    "                result = tx.run(query, ticker = ticker, post_date = post_date, topic_name=topic_name)\n",
    "        return\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDgewbHbn6Ss",
    "outputId": "d0a2c3dc-4d67-4c70-e00f-42b6fa471769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Nodes for Stock:  TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 03:20:56,438 : ERROR : Failed to write data to connection IPv4Address(('1e76e017.databases.neo4j.io', 7687)) (IPv4Address(('34.66.78.163', 7687)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Nodes for Stock: TSLA, Time Taken: 5.46min\n",
      "Making Nodes for Stock:  GM\n",
      "Completed Nodes for Stock: GM, Time Taken: 5.32min\n",
      "Making Nodes for Stock:  STLA\n",
      "0.0136666665\n",
      "Completed Nodes for Stock: STLA, Time Taken: 2.53min\n",
      "Making Nodes for Stock:  HMC\n",
      "0.017225\n",
      "0.015666667000000002\n",
      "Completed Nodes for Stock: HMC, Time Taken: 2.21min\n",
      "Making Nodes for Stock:  RACE\n",
      "Completed Nodes for Stock: RACE, Time Taken: 9.20min\n",
      "Making Nodes for Stock:  TM\n",
      "Completed Nodes for Stock: TM, Time Taken: 3.21min\n",
      "Making Nodes for Stock:  TTM\n",
      "Completed Nodes for Stock: TTM, Time Taken: 9.91min\n",
      "Making Nodes for Stock:  NIO\n",
      "Completed Nodes for Stock: NIO, Time Taken: 8.71min\n"
     ]
    }
   ],
   "source": [
    "iex_key = 'sk_f4ff43b754ef4e7a9f7d21ce5569ef7c'\n",
    "#Neo4j Aura DB Credentials\n",
    "#uri = \"neo4j+s://08b155ba.databases.neo4j.io\"\n",
    "#user = 'neo4j'\n",
    "#password = 'MhrmtDI5RJz7Kx7L8E7tT8qp6eiQ6joClHABFpxYcPE'\n",
    "\n",
    "#iex_key = 'sk_f4ff43b754ef4e7a9f7d21ce5569ef7c'\n",
    "#Neo4j Aura DB Credentials\n",
    "uri = \"neo4j+s://1e76e017.databases.neo4j.io\"\n",
    "user = 'neo4j'\n",
    "password = 'Y-mcrOhxhLiaxChRli6zcnkAWS5NH6gWP2jQ-3X80uc'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = App(uri, user, password)\n",
    "    #app.create_stock('F', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('TSLA', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month =11, day = 30, year = 2021))\n",
    "    app.create_stock('GM', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month =11, day = 30, year = 2021))\n",
    "    app.create_stock('STLA', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('HMC', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('RACE', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('TM', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('TTM', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month = 11, day = 30, year = 2021))\n",
    "    app.create_stock('NIO', dt.datetime(month = 1, day = 1, year = 2021), dt.datetime(month =11, day = 30, year = 2021))\n",
    "    app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYVYASEj-d9V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKrqsHptrieX",
    "outputId": "37623ca6-4861-478c-9dc0-ed724f586a4b"
   },
   "outputs": [],
   "source": [
    "iex_key = 'sk_f4ff43b754ef4e7a9f7d21ce5569ef7c'\n",
    "#Neo4j Aura DB Credentials\n",
    "uri = \"neo4j+s://08b155ba.databases.neo4j.io\"\n",
    "user = 'neo4j'\n",
    "password = 'MhrmtDI5RJz7Kx7L8E7tT8qp6eiQ6joClHABFpxYcPE'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Aura queries use an encrypted connection using the \"neo4j+s\" URI scheme\n",
    "    app = App(uri, user, password)\n",
    "    res = app.run_query('MATCH (e:Event) -[:PriceOf]-> (s:Stock {ticker: \"F\"}) RETURN e, s')\n",
    "    app.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpRxZgH4kBP8"
   },
   "source": [
    "## Pulling tweets from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUeNDN2qTWp0"
   },
   "source": [
    "Run Python with local Runtime (for snscrape)\n",
    "\n",
    "**In local anaconda environment run:**\n",
    "<br>\n",
    "pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "<br>\n",
    "pip install jupyter_http_over_ws\n",
    "<br>\n",
    "jupyter serverextension enable --py jupyter_http_over_ws\n",
    "<br>\n",
    "**Run the following command to open jupyter notebook, allowing colab connection**\n",
    "<br>\n",
    "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --Notebook.port_retries=0\n",
    "\n",
    "\n",
    "In top right of colab notebook click the down arrow and click \"connect to local runtime. Paste link from anaconda prompt in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NT6GAasIVZ7h",
    "outputId": "ccd55c18-8140-4ca1-835a-011cd3bf8f91"
   },
   "outputs": [],
   "source": [
    "!python3 --version\n",
    "#make sure you have 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tzFRmlbTcXd"
   },
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fswPQwYvESbu"
   },
   "outputs": [],
   "source": [
    "# List of companies we want to pull \n",
    "companies = ['Ferrari']#, 'Toyota', 'Honda', 'Stellantis', 'Ford', 'Tesla', 'Ferrari NV' ]\n",
    "company_tickers = ['$RACE']#, '$TM', '$hmc', '$STLA', '$F', '$TSLA', '$RACE']\n",
    "\n",
    "sdate = dt.datetime(month = 11, day = 22, year = 2021, tzinfo=dt.timezone.utc)\n",
    "edate = sdate + dt.timedelta(days=1)\n",
    "\n",
    "since_date = sdate.strftime(\"%Y-%m-%d\")\n",
    "until_date = edate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "company_tweets_df = pd.DataFrame()\n",
    "\n",
    "for company in range(len(companies)):\n",
    "    tweets_list = []\n",
    "    \n",
    "    tweet_keys = [\"date\", \"tweet_id\", \"content\", \"username\", \"company\", \"company_ticker\", \"lang\"]\n",
    "\n",
    "    company_input_string = \"{} since:{} until:{}\".format(companies[company], since_date, until_date)\n",
    "    \n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(company_input_string, top=True).get_items()):\n",
    "      if i>500:\n",
    "        break\n",
    "      #if \"stock\" in tweet.content or \"stock price\" in tweet.content:\n",
    "      tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, companies[company], company_tickers[company], tweet.lang])\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns = tweet_keys)\n",
    "    #tweets_df = tweets_df.loc[tweets_df['date'] > (sdate + dt.timedelta(hours = 2))]\n",
    "\n",
    "    # Gets rid of duplicate tweets (by tweet_id) & keep only last instance\n",
    "    tweets_df = tweets_df.drop_duplicates(subset=['tweet_id'], keep='last')\n",
    "\n",
    "    company_tweets_df = company_tweets_df.append(tweets_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "N3zBt_UBre3G",
    "outputId": "cdc91245-3a06-4b82-d0d9-abb69834c32f"
   },
   "outputs": [],
   "source": [
    "company_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbgf3HC8FrMW"
   },
   "outputs": [],
   "source": [
    "company_tweets_df.to_csv(\"Downloads/test_tweet_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixlQWipqGDIZ"
   },
   "source": [
    "# Using Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_x2rBiP-TUr"
   },
   "outputs": [],
   "source": [
    "#remove duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws5Ma9FwdjhT"
   },
   "outputs": [],
   "source": [
    "def pull_tweets(i, sdate, edate, companies, company_tickers):\n",
    "  company_input_string = \"{} since:{} until:{}\".format(companies[i], sdate, edate)\n",
    "  company_ticker_input_string = \"{} since:{} until:{}\".format(company_tickers[i], sdate, edate)\n",
    "  tweets_list = []\n",
    "\n",
    "  for j, tweet in enumerate(sntwitter.TwitterSearchScraper(company_input_string).get_items()):\n",
    "    if \"stock\" in tweet.content or \"price\" in tweet.content:\n",
    "      tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username, companies[i], company_tickers[i]])\n",
    "\n",
    "  for j, tweet in enumerate(sntwitter.TwitterSearchScraper(company_ticker_input_string).get_items()):\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username, companies[i], company_tickers[i]])\n",
    "\n",
    "  return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "-xPan385dMtt",
    "outputId": "8206c415-4913-403d-cb55-e2ac4a4395c8"
   },
   "outputs": [],
   "source": [
    "# List of companies we want to pull \n",
    "start = time.time()\n",
    "companies = ['GM' ]\n",
    "company_tickers = ['$GM']\n",
    "\n",
    "since_date = '2021-01-13'\n",
    "until_date = '2021-01-14'\n",
    "\n",
    "company_tweets_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "\n",
    "tweet_keys = [\"date\", \"tweet_id\", \"content\", \"username\", \"company\", \"company_ticker\"]\n",
    "\n",
    "\"\"\"\n",
    "company_input_string = \"{} since:{} until:{}\".format(companies[company], since_date, until_date)\n",
    "company_ticker_input_string = \"{} since:{} until:{}\".format(company_tickers[company], since_date, until_date)\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(company_input_string).get_items()):\n",
    "\n",
    "  if \"stock\" in tweet.content or \"price\" in tweet.content:\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username, companies[company], company_tickers[company]])\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(company_ticker_input_string).get_items()):\n",
    "  tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username, companies[company], company_tickers[company]])\n",
    "\"\"\"\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(8) as executor:\n",
    "    responses = list(executor.map(pull_tweets, [x for x in range(len(companies))], [since_date for x in range(len(companies))], [until_date for x in range(len(companies))], companies, company_tickers))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time Taken: \", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi7l1kbTfP7r"
   },
   "outputs": [],
   "source": [
    "for res in responses:\n",
    "    tweets_df = pd.DataFrame(res, columns = tweet_keys)\n",
    "    company_tweets_df = company_tweets_df.append(tweets_df, ignore_index=True)\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns = tweet_keys)\n",
    "\n",
    "# Gets rid of duplicate tweets (by tweet_id) & keep only last instance\n",
    "tweets_df = tweets_df.drop_duplicates(subset=['tweet_id'], keep='last')\n",
    "\n",
    "#company_tweets_df.append(tweets_df, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neo4J Integration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
